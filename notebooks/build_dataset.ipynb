{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twittet dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add RoBerta tokenizer --> DONE\n",
    "Test Roberta tokenizer on a single row --> DONE\n",
    "Check tokenization of strange things (e.g. don't, it's, @, https:/..., www, uppercase). If OK cleaning is Useless\n",
    "Test batch processing when joining multiple rows together: decide padding and truncation --> DONE\n",
    "Understand and test attention mask --> DONE: both MaskedCausalLM or CausalLM. DataCollator does text shift for lables and attention mask (different depending on if CausalLM or MaskedCausalLM)\n",
    "Understand if DataCollator does all the job (shuffling, concatenating, tokenizing text) or we need to merge text and tokenize before --> creating chunks before data collators is important if text lengh > maximum text lenght. If only different text lenght, dynamic padding will deal with it\n",
    "Check max text lenght ofr Twitter dataset --> 549\n",
    "Data processing on \"custom\" Twitter dataset: creating chunks and then DataCollator --> DONE\n",
    "Check if possible to implement shuffle of the merged text--> HF does sampling at each epoch using PyThorc dataset sampler (https://discuss.huggingface.co/t/how-to-ensure-the-dataset-is-shuffled-for-each-epoch-using-trainer-and-datasets/4212)\n",
    "Check train test split make sense --> YES. create val (for fine tuning hps) and test set\n",
    "Restructure lm_datasets to be able to input into datacollator a custom HF dataset (list of dictionary instead) that contains input_ids, attention_mask and labels --> DONE\n",
    "Fine-tuning on MaskedCausalLM: merge text + DataCollator + train_test_split -- DONE\n",
    "Improve script of training and eval for MLM -- DONE\n",
    "Fine-tuning for classification: custom dataset + train_test_split -- DONE\n",
    "Uniform processing scripts along the two steps (how to shuffle data, random seed, how to input in DataSet modul etc.) --DONE\n",
    "Add train end eval script for Sentiment --DONE\n",
    "Review loss and accuracy calculation for both. Add also the use of val (during training) and test (during final evaluation)-- DONE\n",
    "Add local model saving and loading (1st model to be used as input in 2nd step) -- DONE\n",
    "Investigate Roberta main HPs and performance KPIs --> we will keep ppl as MLM metric, and no change in HPs regarding model architecture\n",
    "Adding training logs: inputs parameters, ML performance KPIs (ppl for MLM) + swe performance KPI (training time) -- DONE\n",
    "Pytorch fine-tuning with small dataset to test everything. -- DONE\n",
    "Bring everything on MLFlow -- DONE\n",
    "Create repo ( model + data) on HF Hub open source -- DONE\n",
    "Create model data card V1 in HFHub for 1st step -- DONE\n",
    "Use versioning (given by model card) in tensorboard to differentiate experiment -- DONE\n",
    "Find space to checkpoint models and tfevents (when I'll train on Ec2 instance) -- DONE\n",
    "Add all necessary hparams for both steps (e.g. optimizer name) -- DONE\n",
    "Checkpoint only run_id + best model (with metadata about best epoch) + tokenizer + tflogs -- DONE\n",
    "Cleanup and refactoring -- DONE\n",
    "End-to-end test -- DONE\n",
    "Add tansorboard logging also infra step (with fine tuning we have not many epochs) -- DONE\n",
    "dockerize -- WIP\n",
    "    finish dockerization\n",
    "    create a tech debt/improvents section and add proposal of using S3 (both as mount to use datasets and for uploading trained models)\n",
    "    specify how model training section with docker works in my bot architecture\n",
    "Train on cloud 1st model\n",
    "Push best 1st to HFHub and train 2nd model\n",
    "Script for saving model from EC2 machine back to mu laptop\n",
    "Refactor based on article about how do I setup my ML projects\n",
    "Create central GitHub repo all models (both 1st and 2nd step) (as did be them https://github.com/cardiffnlp/timelms)\n",
    "small script for pushing selected best model to HFHub (training script save only models locally)\n",
    "Add on HF eval metrics, reference to GitHub, hyperparam and any tag to consider each card as a different model version (e.g. \"no processing\")\n",
    "test against TweetBert and this model https://huggingface.co/blog/sentiment-analysis-twitter\n",
    "If TweetBert is better, wrap it around torch to be served\n",
    "Fine-tune chosen model hps (like batch, max len) on new Twitter data\n",
    "Test lr+lr scheduler, different batch size (https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b)\n",
    "Wrap-up: code cleaning and refactoring (keep only code to train and notebook of analysis)\n",
    "Push to GitHub (code) and to HFHub (best model + data)\n",
    "Build API (how to load the model/tokenizer in Pytorch (.tar with state dict https://pytorch.org/tutorials/beginner/saving_loading_models.html))\n",
    "\n",
    "NEXT STEPS\n",
    "Add Gantry and Grafana for prod logs\n",
    "Add logs for inference ML in prod (train-serving skew, data drift, uptime, latency, thorughtput) \n",
    "Logs idea: how many inputs are impacted by MAX_LEN in prod? Useful since hp tuned by training only\n",
    "Switch to W&B for experiment tracking\n",
    "Validation: against fear-greed index\n",
    "Analysis: Test if cleaning processing (copy from article) improve performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soluton for MLM:\n",
    "- tokenize and preprocess the dataset in once Group texts together to obtain chunks of the same lenght and then restructure it for 2nd step\n",
    "- apply data collator at row level (specific dataset class) to obtain masked tokens and labels\n",
    "Comment: Do all processing in once ease the implementation (we should have always chunked text before to extract max of info) since we are doing batch training for MLM. This is not happening in production for inference.\n",
    "\n",
    "Solution for Sentiment Classification:\n",
    "- tokenize the dataset at row level (specific dataset class). Truncation + padding applied. No data collator for simplify things\n",
    "Comment: here processing on the fly at row level ease the inference later on.\n",
    "\n",
    "The two solution can have in common the application of the data collator (two different, btw) at batch level. I can think about it when I'll implement it for sentiment classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreatamburri/miniforge3/envs/transformer/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TARGET_NAME = \"sentiment\"\n",
    "TEXT_NAME = \"text\"\n",
    "DATASET_PATH = '/Users/andreatamburri/Desktop/Sentiment/training.1600000.processed.noemoticon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "df = pd.read_csv(DATASET_PATH, delimiter=',',\n",
    "encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "#df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/andreatamburri/PycharmProjects/SentimentClassifier/TwitterSentiment140/training.1600000.processed.noemoticon.csv',\n",
    "                 delimiter=',',\n",
    "encoding=DATASET_ENCODING , names=DATASET_COLUMNS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning for MLM script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all text in chunk of size chunk_size\n",
    "def group_texts(examples, chunk_size):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# it accepts an HF tokenized dataframe as input\n",
    "# and converts it in a list of Dicts good for HF data collator\n",
    "def refactor_dataset(dataset):\n",
    "    refactored_set = [{\"input_ids\": dataset[\"input_ids\"][i],\n",
    "            \"attention_mask\": dataset[\"attention_mask\"][i],\n",
    "            \"labels\": dataset[\"labels\"][i]   #not necessary for MLM, but is used for CLM, so we keep it\n",
    "            } for i in range(len(dataset[\"input_ids\"]))]\n",
    "    \n",
    "    return refactored_set\n",
    "\n",
    "# Custom Dataset\n",
    "# doesn't tokenize on the fly. We have to process all text toghether before feeding the models to create chunks. \n",
    "# inside tokenization makes sense for sentiment\n",
    "class MLMData(Dataset):\n",
    "    def __init__(self, encodings, data_collator):\n",
    "        # encoding tokenized and chunked toghether as input for CLM/MLM\n",
    "        self.encoding = encodings\n",
    "        self.data_collator = data_collator\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoding)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # it returns a single row of the list of dictionaries converted as tensor objects.\n",
    "        # Keys are:\n",
    "        #   input_ids\n",
    "        #   attention_mask\n",
    "        #   labels\n",
    "        collated_data = self.data_collator([self.encoding[index]])\n",
    "        # data collator need a list to be passed as input and return a dict with values as list of lists\n",
    "        # since we feed only one row the list of lists contains only a lsit\n",
    "        # we need to squeeze it taking only the inside list, otherwise the model fails since accepts as inputs tensor.shape(batch, max_len)\n",
    "        item = {key: val[0] for key, val in collated_data.items()}\n",
    "        #item = {key: val for key, val in self.encoding[index].items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model: nn.Module,\n",
    "          train_loader: DataLoader,\n",
    "          optimizer: torch.optim,\n",
    "          lr: torch.float,\n",
    "          epoch: int,\n",
    "          device: torch.device):\n",
    "\n",
    "    log_interval = 5000\n",
    "    model.train()\n",
    "    tr_loss=0\n",
    "    nb_tr_steps=0\n",
    "    start_time = time.time()\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for n_batch, batch in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # CrossEntropy loss is the first output according to HF docs\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        if n_batch%log_interval==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {n_batch:5d}/{num_batches:5d} batches | '\n",
    "                    f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                    f'loss {loss_step:5.2f}')\n",
    "            start_time = time.time()\n",
    "            print(f\"Avg Training Loss per 5000 steps: {loss_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "# model evaluation\n",
    "def evaluate(model: nn.Module,\n",
    "            testing_loader: DataLoader,\n",
    "            device: torch.device):\n",
    "    model.eval()\n",
    "    tr_loss=0\n",
    "    nb_tr_steps=0\n",
    "    start_time = time.time()\n",
    "    num_batches = len(testing_loader)\n",
    "    with torch.no_grad():\n",
    "        for n_batch, batch in tqdm(enumerate(testing_loader, 0)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # loss (CrossEntropy loss) is the first output according to HF docs\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_steps += 1\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "CHUNK_SIZE = 128\n",
    "OPTIMIZER_NAME = 'AdamW'\n",
    "SCHEDULER = ''\n",
    "\n",
    "\n",
    "# inputs\n",
    "# TODO: missing: github central repo\n",
    "HF_USER = 'andrea-t94'\n",
    "HF_TOKEN = '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'\n",
    "GITHUB_USER = ''\n",
    "MODEL_VERSION_MLM = 'roberta-fine-tuned-twitter'\n",
    "DATASET_VERSION = 'TwitterSentiment140'\n",
    "\n",
    "\n",
    "# params to tfhub\n",
    "params_to_log = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'eval_batch_size': EVAL_BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'optimizer_name': OPTIMIZER_NAME,\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'model': MODEL_VERSION_MLM,\n",
    "    'dataset': DATASET_VERSION\n",
    "    #'scheduler': scheduler\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization\n",
      "merge text in chunks\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "train_texts, val_texts, _, _ = train_test_split(df[TEXT_NAME], df[TEXT_NAME], test_size=.2, random_state=42)\n",
    "train_texts, test_texts, _, _ = train_test_split(train_texts, train_texts, test_size=.15, random_state=33)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', truncation=False, padding=False, do_lower_case=True)\n",
    "\n",
    "# tokenization\n",
    "print('tokenization')\n",
    "tokenized_train = tokenizer(train_texts.to_list())\n",
    "tokenized_val = tokenizer(val_texts.to_list())\n",
    "tokenized_test = tokenizer(test_texts.to_list())\n",
    "\n",
    "# merging\n",
    "print('merge text in chunks')\n",
    "lm_train = refactor_dataset(group_texts(tokenized_train, chunk_size=CHUNK_SIZE))\n",
    "lm_val = refactor_dataset(group_texts(tokenized_val, chunk_size=CHUNK_SIZE))\n",
    "lm_test = refactor_dataset(group_texts(tokenized_test, chunk_size=CHUNK_SIZE))\n",
    "\n",
    "# DataLoader w/ DataCollator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "train_loader = DataLoader(MLMData(lm_train, data_collator), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(MLMData(lm_val, data_collator), batch_size=EVAL_BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(MLMData(lm_test, data_collator), batch_size=EVAL_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |     0/    4 batches | lr 0.00 | ms/batch  0.28 | loss  4.07\n",
      "Avg Training Loss per 5000 steps: 4.074849605560303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:16,  4.09s/it]\n",
      "3it [00:01,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time:  0.00s | train loss  3.90 | valid loss  3.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing best model at epoch   0 | best valid loss  3.63 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     0/    4 batches | lr 0.00 | ms/batch  0.28 | loss  3.70\n",
      "Avg Training Loss per 5000 steps: 3.6995208263397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:17,  4.28s/it]\n",
      "3it [00:01,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.00s | train loss  3.66 | valid loss  3.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing best model at epoch   1 | best valid loss  3.19 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |     0/    4 batches | lr 0.00 | ms/batch  0.29 | loss  3.46\n",
      "Avg Training Loss per 5000 steps: 3.456014633178711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:16,  4.17s/it]\n",
      "3it [00:01,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.00s | train loss  3.43 | valid loss  3.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing best model at epoch   2 | best valid loss  3.04 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |     0/    4 batches | lr 0.00 | ms/batch  0.29 | loss  3.51\n",
      "Avg Training Loss per 5000 steps: 3.5052337646484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:15,  3.80s/it]\n",
      "3it [00:01,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.00s | train loss  3.44 | valid loss  3.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing best model at epoch   3 | best valid loss  3.00 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |     0/    4 batches | lr 0.00 | ms/batch  0.27 | loss  3.27\n",
      "Avg Training Loss per 5000 steps: 3.272932767868042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:15,  3.84s/it]\n",
      "3it [00:01,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.00s | train loss  3.26 | valid loss  3.04\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of training | time:  0.72s | best valid loss  3.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-63f48fba-54c472981b7127ba4c7b732b)\n\nRepository Not Found for url: https://huggingface.co/api/repos/create.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/huggingface_hub-0.11.1-py3.8.egg/huggingface_hub/utils/_errors.py:239\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m writer\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     69\u001b[0m \u001b[39m# I save the best model on each run and overwrite the existing one. If that overperform the one in the main I substitute\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m best_model\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mHF_USER\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mMODEL_VERSION_MLM\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, use_auth_token\u001b[39m=\u001b[39;49mHF_USER)\n\u001b[1;32m     71\u001b[0m tokenizer\u001b[39m.\u001b[39mpush_to_hub(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHF_USER\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mMODEL_VERSION_MLM\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, use_auth_token\u001b[39m=\u001b[39mHF_USER)\n\u001b[1;32m     72\u001b[0m \u001b[39m# save tokenizer vocabulary of my custom model for using for inference\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/transformers/utils/hub.py:779\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, use_auth_token, max_shard_size, create_pr, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     working_dir \u001b[39m=\u001b[39m repo_id\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 779\u001b[0m repo_id, token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_repo(\n\u001b[1;32m    780\u001b[0m     repo_id, private\u001b[39m=\u001b[39;49mprivate, use_auth_token\u001b[39m=\u001b[39;49muse_auth_token, repo_url\u001b[39m=\u001b[39;49mrepo_url, organization\u001b[39m=\u001b[39;49morganization\n\u001b[1;32m    781\u001b[0m )\n\u001b[1;32m    783\u001b[0m \u001b[39mif\u001b[39;00m use_temp_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     use_temp_dir \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(working_dir)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/transformers/utils/hub.py:661\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[0;34m(self, repo_id, private, use_auth_token, repo_url, organization)\u001b[0m\n\u001b[1;32m    658\u001b[0m         repo_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00morganization\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m token \u001b[39m=\u001b[39m HfFolder\u001b[39m.\u001b[39mget_token() \u001b[39mif\u001b[39;00m use_auth_token \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m use_auth_token\n\u001b[0;32m--> 661\u001b[0m url \u001b[39m=\u001b[39m create_repo(repo_id\u001b[39m=\u001b[39;49mrepo_id, token\u001b[39m=\u001b[39;49mtoken, private\u001b[39m=\u001b[39;49mprivate, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    663\u001b[0m \u001b[39m# If the namespace is not there, add it or `upload_file` will complain\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m repo_id \u001b[39mand\u001b[39;00m url \u001b[39m!=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/huggingface_hub-0.11.1-py3.8.egg/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/huggingface_hub-0.11.1-py3.8.egg/huggingface_hub/utils/_deprecation.py:31\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     33\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[1;32m     36\u001b[0m ]\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/huggingface_hub-0.11.1-py3.8.egg/huggingface_hub/hf_api.py:1546\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk)\u001b[0m\n\u001b[1;32m   1543\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(path, headers\u001b[39m=\u001b[39mheaders, json\u001b[39m=\u001b[39mjson)\n\u001b[1;32m   1545\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     hf_raise_for_status(r)\n\u001b[1;32m   1547\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1548\u001b[0m     \u001b[39mif\u001b[39;00m exist_ok \u001b[39mand\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m409\u001b[39m:\n\u001b[1;32m   1549\u001b[0m         \u001b[39m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/transformer/lib/python3.10/site-packages/huggingface_hub-0.11.1-py3.8.egg/huggingface_hub/utils/_errors.py:268\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepoNotFound\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m:\n\u001b[1;32m    260\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    261\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n\u001b[1;32m    271\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    272\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request for \u001b[39m\u001b[39m{\u001b[39;00mendpoint_name\u001b[39m}\u001b[39;00m\u001b[39m endpoint:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m endpoint_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    274\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-63f48fba-54c472981b7127ba4c7b732b)\n\nRepository Not Found for url: https://huggingface.co/api/repos/create.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"training on {device}\")\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Adam with weight decays embedded, lr scheduler?\n",
    "if OPTIMIZER_NAME == 'AdamW':\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER_NAME == 'Adam':\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "else:\n",
    "    print('No known optimizer has been selected. Options are ...')\n",
    "\n",
    "\n",
    "# build tf_log_dir (taken directly from source code)\n",
    "run_id = datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname()\n",
    "tf_log_dir = os.path.join(f'runs/{MODEL_VERSION_MLM}/', run_id)\n",
    "writer = SummaryWriter(tf_log_dir)\n",
    "# model path\n",
    "model_path = os.path.join(f'models/{MODEL_VERSION_MLM}/', run_id)\n",
    "\n",
    "# training\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, lr=LEARNING_RATE, epoch=epoch, device=device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    epoch_start_time = time.time()\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'train loss {train_loss:5.2f} | valid loss {val_loss:5.2f}')\n",
    "    print('-' * 89)\n",
    "    \n",
    "    # checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print('-' * 89)\n",
    "        print(f'| checkpointing best model at epoch {epoch:3d} | best valid loss {best_val_loss:5.2f} ')\n",
    "        print('-' * 89)\n",
    "        best_model.save_pretrained(f'{model_path}/best_model/')\n",
    "        # save everything else needed for resuming training\n",
    "        os.makedirs(f'{model_path}/best_model/checkpoint', exist_ok = True) \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss\n",
    "            }, os.path.join(f'{model_path}/best_model/checkpoint', 'model_sentiment.tar'))\n",
    "        \n",
    "\n",
    "\n",
    "test_loss = evaluate(best_model, test_loader, device)\n",
    "elapsed = time.time() - epoch_start_time\n",
    "print('-' * 89)\n",
    "print(f'| end of training | time: {elapsed:5.2f}s | '\n",
    "        f'best valid loss {best_val_loss:5.2f}')\n",
    "print('-' * 89)\n",
    "\n",
    "writer.add_hparams(params_to_log, {'hparam/test_loss': test_loss}, run_name='.')\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# I save the best model on each run and overwrite the existing one. If that overperform the one in the main I substitute\n",
    "best_model.push_to_hub(f'{HF_USER}/{MODEL_VERSION_MLM}', use_auth_token=HF_USER)\n",
    "tokenizer.push_to_hub(f'{HF_USER}/{MODEL_VERSION_MLM}', use_auth_token=HF_USER)\n",
    "# save tokenizer vocabulary of my custom model for using for inference\n",
    "tokenizer.save_pretrained(f'{model_path}/tokenizer/')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning for Sentiment Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "# Tokenize, pad and truncate on the fly\n",
    "# Since it's a binary classificator, map labels [0,4] --> [0,1]\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, text, sentiment, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text.to_list()\n",
    "        self.targets = sentiment.to_list()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "             # maps [0,4] --> [0,1]\n",
    "            'labels': (torch.tensor(self.targets[index]) > torch.tensor(2)).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self, model_path, dropout):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(model_path)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "# evaluation\n",
    "def evaluate(model: nn.Module,\n",
    "            loss_function: nn.Module,\n",
    "            testing_loader: DataLoader,\n",
    "            device: torch.device):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    num_batches = len(testing_loader)\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for n_batch, batch in tqdm(enumerate(testing_loader, 0)):\n",
    "            input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            tr_loss += loss.item()\n",
    "            # We take the max logits. If I need probabilities I should use SoftMax final layer only for inferecne\n",
    "            pred, pred_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(pred_idx, labels)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=labels.size(0)\n",
    "\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_acc = (n_correct*100)/nb_tr_examples\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# training\n",
    "def train(model: nn.Module,\n",
    "          loss_function: nn.Module,\n",
    "          train_loader: DataLoader,\n",
    "          optimizer: torch.optim,\n",
    "          lr: torch.float,\n",
    "          epoch: int,\n",
    "          device: torch.device) -> None:\n",
    "\n",
    "    log_interval = 5000\n",
    "    model.train()\n",
    "    tr_loss=0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    start_time = time.time()\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for n_batch, batch in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # CrossEntropy loss is the first output according to HF docs\n",
    "        loss = loss_function(outputs, labels)\n",
    "        tr_loss += loss.item()\n",
    "        #to get the prediction value and idx\n",
    "        pred, pred_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(pred_idx, labels)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=labels.size(0)\n",
    "\n",
    "        if n_batch%log_interval==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            acc_step = (n_correct*100)/nb_tr_examples \n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {n_batch:5d}/{num_batches:5d} batches | '\n",
    "                    f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                    f'loss {loss_step:5.2f} | accuracy {acc_step:5.2f}')\n",
    "            start_time = time.time()\n",
    "            print(f\"Avg Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Avg Training Accuracy per 5000 steps: {acc_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_acc = (n_correct*100)/nb_tr_examples\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_LEN = 256\n",
    "DROPOUT = 0.3\n",
    "OPTIMIZER_NAME = 'Adam'\n",
    "SCHEDULER = ''\n",
    "\n",
    "\n",
    "# inputs\n",
    "# TODO: missing: github central repo\n",
    "HF_USER = 'andrea-t94'\n",
    "HF_TOKEN = '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'\n",
    "GITHUB_USER = ''\n",
    "MODEL_VERSION_CLF = 'roberta-fine-tuned-twitter-sentiment'\n",
    "DATASET_VERSION = 'TwitterSentiment140'\n",
    "\n",
    "\n",
    "# params to tfhub\n",
    "params_to_log = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'eval_batch_size': EVAL_BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'optimizer_name': OPTIMIZER_NAME,\n",
    "    'max_len': MAX_LEN,\n",
    "    'dropout': DROPOUT,\n",
    "    'model': MODEL_VERSION_CLF,\n",
    "    'dataset': DATASET_VERSION\n",
    "    #'scheduler': scheduler\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train test split\n",
    "train_texts, val_texts, train_sentiment, val_sentiment = train_test_split(df[TEXT_NAME], df[TARGET_NAME], test_size=.2, random_state=10)\n",
    "train_texts, test_texts, train_sentiment, test_sentiment = train_test_split(train_texts,  train_sentiment, test_size=.15, random_state=9)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(f'{HF_USER}/{MODEL_VERSION_MLM}', truncation=False, padding=False, do_lower_case=True)\n",
    "\n",
    "# custom DataLoader\n",
    "train_loader = DataLoader(SentimentData(train_texts, train_sentiment, tokenizer=tokenizer, max_len=MAX_LEN), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(SentimentData(val_texts, val_sentiment, tokenizer=tokenizer, max_len=MAX_LEN), batch_size=EVAL_BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(SentimentData(test_texts, test_sentiment, tokenizer=tokenizer, max_len=MAX_LEN), batch_size=EVAL_BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 677/677 [00:00<00:00, 259kB/s]\n",
      "Downloading: 100%|██████████| 329M/329M [00:41<00:00, 7.83MB/s] \n",
      "Some weights of the model checkpoint at andrea-t94/roberta-fine-tuned-twitter were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at andrea-t94/roberta-fine-tuned-twitter and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |     0/   43 batches | lr 0.00 | ms/batch  0.20 | loss  0.65 | accuracy 75.00\n",
      "Avg Training Loss per 5000 steps: 0.649319589138031\n",
      "Avg Training Accuracy per 5000 steps: 75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:20,  1.87s/it]\n",
      "25it [00:05,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time:  0.00s | | train accuracy 98.24 | valid accuracy 100.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing model at epoch   0\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing best model at epoch   0 | best valid accuracy 100.00 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     0/   43 batches | lr 0.00 | ms/batch  0.20 | loss  0.02 | accuracy 100.00\n",
      "Avg Training Loss per 5000 steps: 0.022021912038326263\n",
      "Avg Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:19,  1.84s/it]\n",
      "25it [00:06,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.00s | | train accuracy 100.00 | valid accuracy 100.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing model at epoch   1\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |     0/   43 batches | lr 0.00 | ms/batch  0.19 | loss  0.00 | accuracy 100.00\n",
      "Avg Training Loss per 5000 steps: 0.0047179521061480045\n",
      "Avg Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:19,  1.85s/it]\n",
      "25it [00:06,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.00s | | train accuracy 100.00 | valid accuracy 100.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing model at epoch   2\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |     0/   43 batches | lr 0.00 | ms/batch  0.20 | loss  0.00 | accuracy 100.00\n",
      "Avg Training Loss per 5000 steps: 0.0016716643003746867\n",
      "Avg Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:20,  1.87s/it]\n",
      "25it [00:06,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.00s | | train accuracy 100.00 | valid accuracy 100.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing model at epoch   3\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |     0/   43 batches | lr 0.00 | ms/batch  0.19 | loss  0.00 | accuracy 100.00\n",
      "Avg Training Loss per 5000 steps: 0.001042104559019208\n",
      "Avg Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:19,  1.84s/it]\n",
      "25it [00:05,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.00s | | train accuracy 100.00 | valid accuracy 100.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| checkpointing model at epoch   4\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:03,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of training | time:  4.27s | valid accuracy 100.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/roberta-fine-tuned-twitter-sentiment/Feb13_15-23-47_Andreas-MacBook-Pro.local/tokenizer/tokenizer_config.json',\n",
       " 'models/roberta-fine-tuned-twitter-sentiment/Feb13_15-23-47_Andreas-MacBook-Pro.local/tokenizer/special_tokens_map.json',\n",
       " 'models/roberta-fine-tuned-twitter-sentiment/Feb13_15-23-47_Andreas-MacBook-Pro.local/tokenizer/vocab.json',\n",
       " 'models/roberta-fine-tuned-twitter-sentiment/Feb13_15-23-47_Andreas-MacBook-Pro.local/tokenizer/merges.txt',\n",
       " 'models/roberta-fine-tuned-twitter-sentiment/Feb13_15-23-47_Andreas-MacBook-Pro.local/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"training on {device}\")\n",
    "\n",
    "# I fine-tune the very same model I use above\n",
    "model =RobertaClass(model_path=f'{HF_USER}/{MODEL_VERSION_MLM}', dropout=DROPOUT)\n",
    "model.to(device)\n",
    "\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "if OPTIMIZER_NAME == 'AdamW':\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER_NAME == 'Adam':\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "else:\n",
    "    print('No known optimizer has been selected. Options are ...')\n",
    "\n",
    "\n",
    "# build tf_log_dir (taken directly from source code)\n",
    "run_id = datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname()\n",
    "tf_log_dir = os.path.join(f'runs/{params_to_log[\"model\"]}/', run_id)\n",
    "# model path\n",
    "model_path = os.path.join(f'models/{params_to_log[\"model\"]}/', run_id)\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter(tf_log_dir)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0\n",
    "best_model = None\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, loss_function, train_loader, optimizer, lr=LEARNING_RATE, epoch=epoch, device=device)\n",
    "    val_loss, val_acc = evaluate(model, loss_function, val_loader, device)\n",
    "    epoch_start_time = time.time()\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'| train accuracy {train_acc:5.2f} | valid accuracy {val_acc:5.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    # checkpoint\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print('-' * 89)\n",
    "        print(f'| checkpointing best model at epoch {epoch:3d} | best valid accuracy {best_val_acc:5.2f} ')\n",
    "        print('-' * 89)\n",
    "        os.makedirs(f'{model_path}/best_model', exist_ok = True) \n",
    "        # save everything needed for both inference and resume training\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss\n",
    "            }, os.path.join(f'{model_path}/best_model', 'model_sentiment.tar'))\n",
    "\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model,loss_function, test_loader, device)\n",
    "elapsed = time.time() - epoch_start_time\n",
    "print('-' * 89)\n",
    "print(f'| end of training | time: {elapsed:5.2f}s | '\n",
    "        f'valid accuracy {best_val_acc:5.2f}')\n",
    "print('-' * 89)\n",
    "\n",
    "writer.add_hparams(params_to_log, {'hparam/test_loss': test_loss,\n",
    "                                    'hparam/test_acc': test_acc}, run_name='.')\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# save tokenizer vocabulary of my custom model for using for inference\n",
    "tokenizer.save_pretrained(f'{model_path}/tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fear-greed index BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# take fear & greed index\n",
    "url = 'https://api.alternative.me/fng/?limit=1000&date_format=world'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value_classification</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_until_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>20-09-2022</td>\n",
       "      <td>-1663584523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>19-09-2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>Fear</td>\n",
       "      <td>18-09-2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>17-09-2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>16-09-2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>40</td>\n",
       "      <td>Fear</td>\n",
       "      <td>30-12-2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>37</td>\n",
       "      <td>Fear</td>\n",
       "      <td>29-12-2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>37</td>\n",
       "      <td>Fear</td>\n",
       "      <td>28-12-2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>38</td>\n",
       "      <td>Fear</td>\n",
       "      <td>27-12-2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>39</td>\n",
       "      <td>Fear</td>\n",
       "      <td>26-12-2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    value value_classification   timestamp time_until_update\n",
       "0      23         Extreme Fear  20-09-2022       -1663584523\n",
       "1      21         Extreme Fear  19-09-2022               NaN\n",
       "2      27                 Fear  18-09-2022               NaN\n",
       "3      22         Extreme Fear  17-09-2022               NaN\n",
       "4      20         Extreme Fear  16-09-2022               NaN\n",
       "..    ...                  ...         ...               ...\n",
       "995    40                 Fear  30-12-2019               NaN\n",
       "996    37                 Fear  29-12-2019               NaN\n",
       "997    37                 Fear  28-12-2019               NaN\n",
       "998    38                 Fear  27-12-2019               NaN\n",
       "999    39                 Fear  26-12-2019               NaN\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(r.json()['data'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa0d07a2a15f7b1861a5d55bf82ec9504bea1f2f8420499f30751ec5ce4eb710"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
